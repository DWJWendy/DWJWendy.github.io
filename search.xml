<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>详述机器学习中的损失函数</title>
      <link href="/2019/02/25/%E8%AF%A6%E8%BF%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
      <url>/2019/02/25/%E8%AF%A6%E8%BF%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<hr><table><thead><tr><th style="text-align:left">Class</th><th style="text-align:left">Content</th></tr></thead><tbody><tr><td style="text-align:left">layout</td><td style="text-align:left">post</td></tr><tr><td style="text-align:left">title</td><td style="text-align:left">详解机器学习中的损失函数</td></tr><tr><td style="text-align:left">categories</td><td style="text-align:left">Blog</td></tr><tr><td style="text-align:left">description</td><td style="text-align:left">机器学习中常见的损失函数以及它们的特点和适用场景</td></tr><tr><td style="text-align:left">keywords</td><td style="text-align:left">机器学习 损失函数 风险函数</td></tr></tbody></table><hr><h4 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h4><p>我们知道机器学习的三要素是:方法= 模型+策略+算法, 如何从假设空间中选择最优模型,这涉及到我们需要用什么样的准则进行学习,这就是三要素中的”策略”问题。</p><p>在假设空间中选择模型$y(x_n,w)$作为决策函数,给定输入$x_n$,由模型得到输出$y(x_n,w)$,而预测的$y(x_n,w)$与真实值$t_n$之间可能不一致,如图1-1 可以看出预测值$y(x_n,w)$与真实值$t_n$存在不一致情况,他们之间的差的绝对值为$|y(x_n,w)-t_n|$为绿色线部分, 而损失函数是定义在单个样本上的，算的是一个样本的误差。因此选用损失函数来度量预测误差。<br><img src="https://upload-images.jianshu.io/upload_images/5274272-1cf03d36a2499644.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-1 预测值与真实值的误差"></p><p>损失函数（loss function）是用来度量模型的预测值与真实值的不一致程度，是一个非负实值函数,损失函数越小，预测正确程度越高，表示为：$$L(y_i,f(x_i))$$</p><ul><li><p>损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的<strong>结构风险函数包括了经验风险项和正则项</strong>，可以表示为：$$R_{srm}(f)= \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J (f)$$<br>这个公式为结构风险函数,其中,包括前一部分的经验风险项以及后一部分的正则化项,正则化项用于控制模型复杂度,$\lambda$则是用于权衡经验风险和模型复杂度之间的关系.<br>所以,通过最小化结构风险的策略找到最优模型,求解最优模型就是求解如下最优化问题：$$min_{f\in\digamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$</p></li><li><p>当然,除了让结构风险最小化寻找最优模型外,还可以直接最小化经验风险,即<br>$$min_{f\in\digamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$<br>在样本足够的情况下,经验风险最小化可以达到很好的学习效果,但是样本容量有限时,容易产生过拟合现象,所以在才有上面结构风险最小化求最优模型的策略.</p></li></ul><hr><h4 id="2-区别损失函数-风险函数-代价函数-目标函数"><a href="#2-区别损失函数-风险函数-代价函数-目标函数" class="headerlink" title="2. 区别损失函数 风险函数 代价函数 目标函数"></a>2. 区别损失函数 风险函数 代价函数 目标函数</h4><ul><li><strong>损失函数</strong>：衡量单个样本预测值与真实值的误差【不赘述】.</li><li><strong>代价函数</strong>：定义在训练集上，是模型关于训练集的平均损失，它也叫经验风险，表示为：$$\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$</li><li><strong>风险函数</strong>：是指损失函数的期望,又叫期望损失，由于输入$X$和输出$Y$是随机变量，那么可求得联合分布$P(X,Y)$，所以可以表示为：$$R_{exp}(f)=E_p[L(Y,f(X))] = \int_{X,Y}L(y,f(x))p(x,y)dxdy$$</li><li><strong>目标函数</strong>：是一个更为广的概念，比如最小化结构风险求最优模型时，结构化风险函数就是目标函数，而最小化经验风险求最优模型时，经验风险函数就是目标函数，简单地讲，目标函数就是需要优化的函数。</li></ul><p><strong>Note：</strong></p><ul><li>a.通常，我们没有细分损失函数和代价函数，经常将两个概念混用。</li><li>b.由于$P(Y,X)$未知，所以风险函数无法计算，经验风险$R_{emp}(f)$是模型关于训练集的平均损失，<strong>根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}(f)$趋于风险函数$R_{exp}(f)$</strong>，这也说明了训练集容量越大，选择的最优模型越逼近真实模型。</li></ul><hr><h4 id="3-常见的损失函数"><a href="#3-常见的损失函数" class="headerlink" title="3. 常见的损失函数"></a>3. 常见的损失函数</h4><ul><li><p><strong>(1) 0-1损失函数(Zero-one Loss)</strong><br>$$<br>L(y_i,f(x_i)) =<br>\begin{cases}<br>1,  &amp; \text{$y_i \neq f(x_i)$} \\<br>0, &amp; \text{$y_i = f(x_i)$}<br>\end{cases} $$</p></li><li><p>0-1 损失函数简单易理解，<strong>用于分类</strong>，如果预测的标签和数据标注的标签一致，那么就为0，否则就为1，当然, 如果认为相等的要求太严苛，可以放宽要求，<strong>用于回归</strong>中，如果他们的绝对值小于某个阈值，即为0，否则为1，表示为<br>$$<br>L(y_i,f(x_i)) =<br>\begin{cases}<br>1,  &amp; \text{$|y_i-f(x_i)| \geq t$} \\<br>0, &amp; \text{$|y_i-f(x_i)| &lt; t$}<br>\end{cases} $$</p></li><li><p><strong>(2) 平方损失函数(Square Loss)</strong><br>$$L(y_i,f(x_i))=(y_i-f(x_i))^2$$<br>由于其计算的优越性，所以常常<strong>用于回归</strong>中, 权重由可以直接初始化，再通过梯度下降不断更新。</p></li><li><p><strong>举例说明</strong>，一个线性回归，假设输入有$n$个特征，为了方便表示用$n+1$表示输入向量为$x=[0,x_1,x_2,…,x_n]^T$，模型参数为$w=[w_0,w_1,…,w_n]^T$，那么线性回归的模型表示为$$f(x)=w_0+w_1x_1+,…,w_nx_n=w^Tx$$<br>那么它的代价函数可以表示为$$L(w,x)=\frac{1}{2N}\sum_{i=1}^N(y^{(i)}-w^Tx^{(i)})+\frac{\lambda}{2}||w||^2$$<br>注意：在这里都使用$\frac{1}{2}$的目的是方便在后续求导中计算，$\lambda$是正则项前面的参数。<br>对$w_i$求导：<br>$$\frac{dL(w,x)}{dw_i} = \frac{1}{N}\sum_{i=1}^N(y_i-w^Tx)x_i+\lambda<br>w_i$$<br>由于这是目标函数是一个有最小值的凸函数，所以是沿着梯度的反方向进行更新，表示为$$w_i = w_i -\eta\frac{dL(w,x)}{dw_i}$$<br>注：权重矩阵$w$最开始可以随机初始，再不断更新.</p></li><li><p><strong>(3) 绝对损失函数(Absolute loss)</strong><br>$$L(y_i,f(x_i))=|(y_i-f(x_i))|$$<br>绝对值损失函数也经常<strong>用于回归中</strong>,而用于分类就等价于0-1损失函数，在sklearn中有Huber损失函数，它是平方损失函数和绝对值损失函数的结合，详细介绍请参考 <a href="http://sklearn.apachecn.org/cn/stable/modules/ensemble.html#f2001" target="_blank" rel="noopener">[F2001]</a>，比较简单【不再赘述】。</p></li><li><p><strong>(4) 对数损失函数(Log Loss or Cross-entropy Loss)</strong><br>$$L(y_i,f(x_i))=-logP(y_i|x_i)$$<br>对数损失函数适<strong>用于逻辑回归</strong>,那什么是逻辑回归呢？<br><strong>举例说明：</strong>其实就是在线性回归的基础上增加了逻辑函数$h(x)= \frac{1}{1+e^{-x}}$，那么对于在线性回归基础上加上逻辑函数，则逻辑回归模型可以表示为$$p(1|w,x^{(i)})=h(f(x^{(i)}))=\frac{1}{1+e^{-f(x^{(i)})}}=\frac{1}{1+e^{-w^Tx^{(i)}}}$$<br>即上式表示，给定输入，该实例为类别1的概率，由于逻辑回归是二分类，所以为类别0的概率为$p(0|w,x^{(i)})=1-p(1|w,x^{(i)})$<br>而在逻辑回归中，由于提供了模型的概率估计，所以使用的损失函数为对数损失函数，表示如下：<br>$$<br>L(y^{(i)},f(x^{(i)})) =<br>\begin{cases}<br>-logp(1|w,x^{(i)}),  &amp; \text{$y^{(i)}=1$} \\<br>-log(1-p(w|x^{(i)})), &amp; \text{$y^{(i)}=0$}<br>\end{cases} $$</p></li></ul><p>那么最终的代价函数可以表示为<br>$$L(w,x)=-\frac{1}{N}\sum_{i=1}^N \lbrace y^{(i)}logp(y^{(i)}=1|w,x^{(i)})+(1-y^{(i)})logp(y^{(i)}=0|w,x^{(i)})\rbrace<br>$$<br>接下来就从另外一个角度说明，已知逻辑回归模型表示为：$$p(1|w,x^{(i)})=h(f(x^{(i)}))=\frac{1}{1+e^{-f(x^{(i)})}}=\frac{1}{1+e^{-w^Tx^{(i)}}}$$<br>在模型的数学形式确定后，剩下的就是如何去求解模型中的参数$w$，而在已知模型和一定样本的情况下，估计模型的参数，在统计学中常用的是极大似然估计方法。即找到一组参数$w$，使得在这组参数下，样本数据的似然度（概率）最大.<br>对于逻辑回归模型，假定概率分布服从伯努利分布【0-1分布】，其概率质量函数PMF为:$f(x)=p^x(1-p)^{(1-x)}$，其中$x$只能取0或者1，那么似然函数可以表示:为$$L(w)=\prod_{i=1}^Np(y^{(i)}=1|w,x^{(i)})^{y^{(i)}}p(y^{(i)}=0|w,x^{(i)})^{1-y^{(i)}}$$<br>那么对上式取对数，得到对数似然函数为:<br>$$logL(w)=\sum_{i=1}^Ny^{(i)}logp(y^{(i)}=1|w,x^{(i)})+（1-y^{(i)})logp(y^{(i)}=0|w,x^{(i)})$$<br>则全体样本的代价函数为:<br>$$logL(w)=-\sum_{i=1}^N\lbrace y^{(i)}logp(y^{(i)}=1|w,x^{(i)})+（1-y^{(i)})logp(y^{(i)}=0|w,x^{(i)})\rbrace$$<br>由此可以看出<strong>对数损失函数与极大似然估计的对数似然函数本质上是等价的</strong>.</p><ul><li><strong>(5) 绞链损失函数(Hinge Loss)</strong><br>$$L(m_i)=max(0,1-m_i(w))$$<br>铰链损失函数<strong>常用于支持向量机(SVM)中</strong>,它名字来源于它的损失函数图像为一个折线图像，如果模型分类正确，损失为0，否则损失为$1-m_i(w)$，在SVM损失函数表示为：$$L(y^{(i)},x^{(i)})=max(0,1-y^{(i)}f(x^{(i)}))$$,<br><strong>举例说明：</strong>在SVM中，最终的支持向量机的分类模型是能最大化分类间隔，又减少错误分类的样本数目，意味着一个好的支持向量机的模型，需要满足以上两个条件：<strong>1、最大化分类间隔，2、错误分类的样本数目</strong>。如图说明，$f(x^{(i)})$是预测值，且预测值在-1到1之间，$y^{(i)}$为目标值取值为-1或者1，如果分类正确，$L(y^{(i)},x^{(i)})=0$，如果分类错误，$L(y^{(i)},x^{(i)})=1-y^{(i)}f(x^{(i)})$，$\xi$是引入的松弛变量，错误分类的样本数目，就回到了损失函数的范畴。<br><img src="https://upload-images.jianshu.io/upload_images/5274272-cd3776e65ca5cbc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SVM分类">综上所述推导SVM的代价函数，最初的SVM优化函数如下： </li></ul><p>$$\begin{equation}<br>    \mathop{\arg\min}_{(w,\xi_i)}   | \frac{1}{2} \Vert w \Vert^2+C\sum_i\xi_i \\<br>   \begin{cases}<br>    &amp; s.t.\quad \forall y^{(i)}f(x^{(i)}) \geq 0 \\<br>                &amp; \quad\quad\quad \xi_i \geq 0<br>        \end{cases}<br>\end{equation}$$</p><p>将约束条件变形为$$\xi_i&gt;1-y^{(i)}f(x^{(i)})$$<br>最终SVM的代价函数可以表示为：$$L(w,x^{(i)})=C  \sum_{i=0}^{N}max(0,1-y^{(i)}f(x^{(i)}))+\frac{1}{2} \Vert w \Vert^2$$</p><ul><li><strong>(6) 指数损失函数(Exponential loss)</strong><br>$$L(y_i,f(x_i)) = exp(-y_if(x_i))$$指数损失函数，<strong>主要应用于 Boosting 算法中</strong><br>在Adaboost 算法中，经过$m$次迭代后，可以得到 $f_m(x^{(i)})$，表示为：$$f_m(x^{(i)})=f_{m−1}(x^{(i)})+α_mG_m(x^{(i)})$$<br>Adaboost 每次迭代时的目的都是找到最小化下列式子的参数$α$和$G$：<br>$$argmin_{(α,G)}  \quad \sum_{i=1}^{N}exp(-y^i(f_{m−1}(x^{(i)})+αG(x^{(i)})))$$<br>易知，Adaboost应用了指数损失函数$L(y^i,f(x^i))=exp(-y^if(x^i))$<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4>看了很多资料和网页，结合自己的理解写出了这一篇博客，以前只知道有这些损失函数，并没有探讨过损失函数的适用性以及该如何去推导这些公式。继续加油O(∩_∩)O~~<br>如有问题联系<a href="mailto:dengwenjun818@gmail.com" target="_blank" rel="noopener">dengwenjun818@gmail.com</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> loss function </tag>
            
            <tag> cost function </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法基础概念</title>
      <link href="/2019/02/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/02/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<hr><table><thead><tr><th style="text-align:left">Class</th><th style="text-align:left">Content</th></tr></thead><tbody><tr><td style="text-align:left">layout</td><td style="text-align:left">post</td></tr><tr><td style="text-align:left">title</td><td style="text-align:left">第一章 统计学习方法概论</td></tr><tr><td style="text-align:left">categories</td><td style="text-align:left">Course</td></tr><tr><td style="text-align:left">description:</td><td style="text-align:left">李航 《统计学习方法》的学习, 主要介绍基本概念,详细叙述了监督学习,提出统计学习的三要素:模型\策略\算法\,另外还介绍了模型选择:包括正则化\交叉验证与学习的泛化能力,此外还介绍了生成模型和判别模型.</td></tr><tr><td style="text-align:left">keywords</td><td style="text-align:left">监督学习 模型三要素 模型选择 生成模型 判别模型</td></tr></tbody></table><hr><h4 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h4><p><strong>a. ~概念:</strong></p><p>又叫统计机器学习,人们提到的机器学习往往指统计机器学习, 它是利用<strong>计算机</strong>对数据<strong>构建的概论统计模型</strong>,并运行模型<strong>对新的数据进行预测和分析</strong></p><p><strong>b. ~特点:</strong><br>(1)平台[计算机及网络] ;<br>(2)对象:数据;<br><strong>(3)目的:分析和预测</strong>;<br>(4)中心:统计学习方法;<br>(5)多领域交叉[概率论\统计学\信息论\最优化\计算理论\计算机科学等]</p><p><strong>c. ~前提假设:</strong></p><p><strong>同一类数据具有一定的统计规律性</strong>[因为它具有统计规律,所以可以用概率统计的方法进行处理,eg 用随机变量描述数据的特征,概论分布描述数据的统计规律]</p><p><strong>d. ~方法:</strong> </p><p>统计学习方法可以分为: 监督学习\非监督学习\半监督学习\强化学习等</p><p><strong>e. 统计学习方法三要素:</strong></p><p><strong>方法 = 模型+策略+算法</strong></p><p><strong>模型</strong>: 这里不再赘述<br><strong>策略</strong>: 按照什么准则学习或者选择最优模型,引用了损失函数(loss function)与风险函数(risk function),详细介绍见博文《详解机器学习中的损失函数》.<br><strong>算法</strong>:学习模型的具体计算方法,根据学习策略从假设空间中选择最优模型,最后需要考虑用什么方法计算最优模型,转化为最优化问题,可能出现有显式的解析解,也可能需要用数值计算的方法进行求解.</p><p>注: 假设训练集和测试集数据是独立同分布产生的</p><p><strong>f. ~步骤:</strong></p><ul><li>得到一个有限的训练数据集合;</li><li>确定包含所有可能的模型的<strong>假设空间</strong>,即学习模型的集合; </li><li>确定模型选择的<strong>准则</strong>,即学习的策略; </li><li>实现<strong>求解最优模型的算法</strong>,即学习的算法;</li><li>通过学习选择最优模型; </li><li>利用学习的最优模型对新数据进行分析和预测.</li></ul><p><strong>g. ~重要性</strong> : 统计学习是处理海量数据最强有力的工具,是计算机智能化的有效手段.</p><hr><h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p><strong>a. 监督学习概念:</strong></p><p>是统计学习的方法之一, 它是学习一个模型,使得模型能对任意给定的输入,对其相应的输出做出一个好的预测. [最好的判断方式是看它<strong>是否有标签</strong>]</p><p><strong>b. 输入空间\特征空间\输出空间:</strong></p><p>输入与输出所有可能的取值的集合称为输入空间与输出空间,通常输出空间小于输入空间.特征空间表示所有特征存在的空间,而模型实际上是定义在特征空间上的.</p><p><strong>c.训练集和测试集的表示:</strong></p><p> $X$表示输入随机变量,$Y$表示输出随机变量,$x$和$y$是定义在输入和输出空间上随机变量的取值.<br><strong>训练集</strong>: $Train=\lbrace(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\rbrace$<br><strong>测试集</strong>: $Test={(x_{1}^\ast,y_{1}^\ast),(x_{2}^\ast,y_{2}^\ast),…,(x_{n}^\ast,y_{n}^\ast)}$<br>输入和输出对又称为样本或样本点</p><p><strong>d. 不同预测任务的名称:</strong> </p><p>输入变量和输出变量均为连续的预测问题称为<strong>回归问题</strong>;输出变量为有限个离散变量的预测问题称为<strong>分类问题</strong>;输入变量和输出变量均为变量序列的预测问题被称为<strong>标注问题</strong>.</p><p><strong>注: 对于标注问题,目前还没有做过类似的项目,初步理解如下:</strong><br><strong>标注问题:</strong></p><ul><li>输入序列:  $x_i = (x_{i}^{(1)},x_{i}^{(2)},…,x_{i}^{(n)})^T ,i = 1,…,N$</li><li>输出标记序列:  $y_i = (y_{i}^{(1)},y_{i}^{(2)},…,y_{i}^{(n)})^T ,i = 1,…,N$<br>其中$n$为序列长度,不同的样本可以有不同的长度</li><li>条件概率分布为:$P(Y^{(1)},Y^{(2)},…,Y^{(n)}|X^{(1)},X^{(2)},…,X^{(n)})$</li><li>评价指标与分类问题的一致</li><li>常用的机器学习方法:隐马尔可夫模型\条件随机场等</li><li>应用场景:eg 自然语言词性标注.</li></ul><p>e. 监督学习的基本假设: <strong>监督学习假设输入和输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$</strong>,训练集和测试集被看做上依联合概率分布$P(X,Y)$独立同分布产生的.</p><p><strong>f. 假设空间:</strong></p><p>是指包含所有可能的模型的集合,以监督学习为例,假设空间包含所有可能的条件概率分布或决策函数, 它是有参数向量决定的函数族或者条件概率分布族,表示为<br>$\lbrace f|Y=f_\theta(X), \theta\in R^n\rbrace$ 或者 $\lbrace P|P(Y|X)), \theta\in R^n\rbrace$</p><p>而模型是输入到输出的映射,,学习的目的是找到最好的模型,所有假设空间就确定了学习范围.而监督学习的模型可以是概率模型($P(Y|X)$)也可以是非概率模型$Y=f(X)$,所以对于一特定的实例,输出预测为$P(y|x)$和$y=f(x)$.</p><p><strong>g. 监督学习形式化如下:</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-0cce7952ec884123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-1 监督学习图"></p><p>阐释: 监督学习假设训练集和测试集都是依联合概率分布$P(X,Y)$独立同分布产生的.从训练集出发,学习系统先训练数据, 应用某个评价准则从假设空间里选择最佳的模型,使得模型对训练数据和新数据在给定的评价准则下有最优的预测,学习到的模型表示为$P(Y|X)$或者$Y=f(X)$,所以在预测过程,给定输入$x_{(n+1)}$,由模型得到$y_{(n+1)}^*$</p><hr><h4 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h4><p><strong>a. 训练误差\测试误差\泛化误差的概念:</strong></p><p>统计学习不仅要对训练数据有较好的拟合效果,还要对未知数据有较好的泛化能力. 对于给定的损失函数,基于损失函数的模型的训练误差[针对训练集]和模型的测试误差[针对测试集]是学习方法的评估标准. 测试误差反映模型对未知数据的预测能力,测试误差越小,预测能力,即泛化能力越强.</p><p>注: 1. 学习所采用的损失函数可以与预测采用的损失函数不同,但是一般是一致的.</p><ul><li>训练误差:$$R_{exp}(f) = \frac {1}{N}\sum_{i=1}^{N} L(y_i,y_i^*) $$<br>训练误差为模型$Y=f(X)$关于训练集的平均损失,$N$为训练样本容量</li><li>测试误差: $$R_{test}(f) = \frac {1}{N^{‘}}\sum_{i=1}^{N^{‘}} L(y_i,y_i^*) $$<br>测试误差为模型$Y=f(X)$关于测试集的平均损失,$N^{‘}$为训练样本容量</li><li>泛化误差: 泛化能力是指对未知数据的预测能力,而泛化误差反映了学习方法的泛化能力,泛化误差越小,方法越有效, <strong>它实际上就是模型的期望风险</strong>,可以表示为$$R_{exp}(f) = E_p[L(Y.f(X))]=\int_{X \times Y}L(y,f(x))P(x,y)$$</li></ul><p><strong>举例说明</strong><br>当损失函数为0-1损失时,测试误差为 $$e_{test} = \frac {1}{N^{‘}}\sum_{i=1}^{N^{‘}} I(y_i \neq y_i^\ast) $$ 其中$I$为指示函数,即$y_i \neq y_i^\ast$为1,否则为0.<br>相应地,测试集上的准确率为 $1-e_{test}$</p><p><strong>b. 过拟合和模型选择</strong></p><ul><li>模型选择: 由于假设空间含有不同复杂度的模型,模型选择的目的是选择尽可能最优的模型去逼近真模型.</li><li>过拟合: 为了追求训练集的预测能力,提高模型的复杂度[主要是模型包含过多参数],从而导致模型对<strong>训练集预测效果很好,对未知数据泛化能力很差</strong>.</li></ul><p><strong>举例说明:多项式曲线拟合</strong><br>给定一组训练集,表示为$T =\lbrace(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\rbrace$, 对训练集我用下面的多项式进行拟合:<br>$$y^\ast(x,w)= w_0+w_1x+w_2x^2+…+w_Mx^M= \sum_{j=0}^Mw_j^{x_j}$$<br>$$L(w)=\frac{1}{2}\sum_{n=1}^N\lbrace(y^\ast(x_n,w)-y_n)\rbrace^2$$<br>其中$y^*(x_n,w)$是模型预测结果,$y^n$为原始的正确结果,通过最小二乘法可以求得拟合的多项式系数的唯一解.</p><p>拟合结果如(图1-2),图中给出了M=0,1,3,9的拟合效果,可以看出,当M=0时,多项式曲线是常数,拟合效果很差,M=1时,多项式曲线为直线,拟合效果依然很差,相反M=9,多项式曲线拟合了所以点.对于M=0和3的情况下,表现为欠拟合(即拟合不够), 而M=9为过拟合现象.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-76c6f77c05fa0acd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-2 不同阶数的多项式曲线，⽤红⾊曲线表⽰"></p><p>那么再看一下此时的训练误差和测试误差与模型复杂度关系如(图1-3),随着模型复杂度增加,训练误差和测试误差都先降低,但是随着复杂度变得更高的时候,训练误差变得更小甚至趋近于0,而测试误差陡然上升,此时出现了过拟合现象,模型对训练集过拟合,而对未知数据泛化能力下降.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-ad0a509d1f878c97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-3 预测误差与模型复杂度关系"><br>另外,我们在看一下参数与模型复杂度的关系, 随着模型复杂度增加$||w||$呈现增加趋势,这也是可以通过<strong>正则化控制模型过拟合</strong>的原因.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-b13906f0ccf143eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-4 参数与模型复杂度关系"></p><p><strong>问题来了,那么如何防止过拟合呢?</strong></p><ul><li><strong>1. 增加训练集</strong><br>对于多项式曲线拟合的例子,通过增加训练集数量可以有效减缓过拟合, 可以看出N=15时,模型拟合效果很好,而增加数据,模型难以将每一个数据的都拟合到,所以增加训练集数量可以有效防止过拟合.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-0e3aa8f40386f5a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-5 增加数据集减缓过拟合"></li><li><p><strong>2. 正则化</strong><br>正则化是指在<strong>经验风险</strong>的基础上加上<strong>正则化项</strong>,正则化项可以取不同的形式,可以是参数向量的$L_2$范数,也可以是$L_1$范数,但是一般来说,<strong>正则化项是模型复杂度单调递增函数</strong><br>$$L(w)=\frac{1}{2}\sum_{n=1}^N\lbrace(y^\ast(x_n,w)-y_n)\rbrace^2+\frac{\lambda}{2}||w||^2$$<br>其中,公式前一部分是经验风险,公式后一部分$\frac{\lambda}{2}||w||^2$是正则化项,$||w||^2= w^Tw=w_0^2+w_1^2+…+w_M^2$,通常,系数$w_0$从正则化项中省略，因为包含$w_0$会使得结果依赖于⽬标变量原点的选择,<br>(Hastie et al., 2001), 而$\lambda$的作用是调节经验风险和正则化项关系的系数,而<strong>正则化的作用是选择经验风险和模型复杂度都较小的模型</strong><br>下图是M=9的模型,利用正则抑制过拟合的例子,图左通过加入正则化项,于$\ln\lambda$ = −18，过拟合现象被压制,而图右$\lambda$过大未能调节好经验风险和正则化项关系,模型拟合效果也差.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-fdc11c80f25065f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-6 M=9正则抑制过拟合"><br>⽤M = 9的多项式拟合,拟合的多项式的对应的系数在表1.7中给出，表明正则<br>化在减⼩系数的值⽅⾯产⽣了预期的效果.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-af4bbf5000e198c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-7 λ控制参数"><br><strong>但是$\lambda$该取值多少呢?</strong><br>可以从下图1-8看出,$\lambda$控制了模型的复杂性，因此决定了过拟合的程度.<br><img src="https://upload-images.jianshu.io/upload_images/5274272-d0992528955d1496.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-8. 不同λ训练误差和测试误差的变化"><br>总结: 在损失函数中加入正则项可以有效避免过拟合,而正则项的$\lambda$决定了过拟合程度,因为起为高参,需要做参数实验取得最佳值.而从贝叶斯估计的角度看,正则化项对应于模型的先验概率,复杂的模型先验概率小,简单的模型先验概率大.</p></li><li><p><strong>3. 交叉验证</strong><br>交叉验证的基本思想是重复使用数据,把给定的数据切分成为训练集和测试集,在此基础上反复进行训练]测试以及模型选择.</p></li></ul><p><strong>详细的交叉验证方法以及如何交叉验证见博客《简述机器学习中模型评估方法》</strong></p><hr><h4 id="生成模型和判别模型"><a href="#生成模型和判别模型" class="headerlink" title="生成模型和判别模型"></a>生成模型和判别模型</h4><p>监督学习模型可以分为两种:生成模型(generative model)和判别模型(discriminative model).</p><ul><li>生成模型:由于模型给定了输入$X$和输出$Y$的关系,所以学习到联合概率分布P(X,Y),然后求出条件概率分布$P(Y|X)$作为预测,eg 朴素贝叶斯\隐马尔可夫模型.<br>生成模型表示为:$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$</li><li>判别模型:由数据直接学出决策函数$f(X)$或者条件概率分布$P(Y|X)$,只关心给定输入$X$预测输出$Y$,eg.K-近邻\感知机\决策树\最大熵\SVM等.</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本章主要介绍了机器学习的基本概念,详细讲解了监督学习,训练误差\预测误差\泛化误差,生成模型和判别模型等,文章框架依据李航《统计学习方法》第一章而来,准备地说,是自己对其的归纳和简述,同时举例参考bishop的经典《模式识别和机器学习》书籍.</p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简述机器学习中模型评估方法</title>
      <link href="/2019/02/25/%E7%AE%80%E8%BF%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/25/%E7%AE%80%E8%BF%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<hr><table><thead><tr><th style="text-align:left">Class</th><th style="text-align:left">Content</th></tr></thead><tbody><tr><td style="text-align:left">layout</td><td style="text-align:left">post</td></tr><tr><td style="text-align:left">title</td><td style="text-align:left">简述机器学习中模型的评估方法</td></tr><tr><td style="text-align:left">categories</td><td style="text-align:left">Blog</td></tr><tr><td style="text-align:left">description</td><td style="text-align:left">模型评估方法主要用于对模型的泛化误差进行评估进而选择最优模型，主要的方法有留出法、S折交叉验证法、自助法</td></tr><tr><td style="text-align:left">keywords</td><td style="text-align:left">留出法 交叉验证法 自助法，以及对应的代码</td></tr></tbody></table><hr><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>机器学习的模型选择指如何从假设空间中选择泛化能力最大的模型，增加训练数据集、加正则项都能有效地抑制过拟合现象，而在实际应用中数据是不充足的，为了选择最优的模型还可以通过交叉验证方法。</p><p><strong>交叉验证的思想：</strong>重复使用给定的数据，把给定的数据切分为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。</p><p>当然，如果样本数量充足，可以随机地把数据切分成三部分：训练集用来训练模型，验证集用于模型选择合调参，测试集对最终模型的评估。</p><h4 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h4><p><strong>1. 留出法（简单交叉验证）</strong><br>方法：随机将样本分为测试集和训练集，然后用训练集训练模型，得到训练模型之后，在测试集上评价各个模型的测试误差，再选择测试误差最小的模型。</p><p><strong>Note：</strong><br>a. <strong>训练集和测试集的划分尽量保持数据分布的一致性</strong>，避免因为数据划分过程引入额外的偏差，例如在多分类时至少保证样本的类别比例相似，从采样的角度来看待数据划分，采样方式被称为“分层采样”。<br>b. <strong>使用留出法一般采用若干次随机划分、重复实验评估取平均值</strong>，单次使用留出法结果往往不够稳定。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def simple_cross(item, <span class="keyword">label</span>, k):</span><br><span class="line">    <span class="string">""</span>"</span><br><span class="line">    对每类标签随机抽取相同的个数构造成测试集,样本中剩余的数据为训练集,要求数据为数据框结构,且标签的column为<span class="string">"Label"</span></span><br><span class="line">    :param item: <span class="keyword">sample</span> data</span><br><span class="line">    :param <span class="keyword">label</span>:data <span class="keyword">type</span> of <span class="keyword">list</span>. 标签名称列表,eg <span class="keyword">label</span>=[1,2,3,4,5,6]</span><br><span class="line">    :param k: <span class="keyword">sample</span> number of each <span class="keyword">label</span>. 每类标签的数量</span><br><span class="line">    :<span class="keyword">return</span>: train and <span class="keyword">test</span> data</span><br><span class="line">    <span class="string">""</span>"</span><br><span class="line">    label_indexs, all_indexs = &#123;&#125;, []</span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> <span class="keyword">label</span>:</span><br><span class="line">        label_indexs[_i] = []</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> item.index:</span><br><span class="line">        label_indexs[item.ix[index, <span class="string">"Label"</span>]].<span class="keyword">append</span>(index)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> label_indexs.values():</span><br><span class="line">        all_indexs.extend(random.<span class="keyword">sample</span>(value, k=k))</span><br><span class="line">    test_data = item.ix[all_indexs, ]</span><br><span class="line">    train_data = item.<span class="keyword">drop</span>(all_indexs)</span><br><span class="line">    <span class="keyword">return</span> train_data, test_data</span><br></pre></td></tr></table></figure></p><p><strong>2. S折交叉验证</strong><br>方法：随机将样本拆分成<strong>S</strong>个互不相交大小相同的子集，然后用<strong>S-1</strong>个子集作为训练集训练模型，用剩余的子集测试模型，对<strong>S</strong>中选择重复进行，最终选出<strong>S</strong>次测评中的平均测试误差最小的模型。</p><p><strong>Note：</strong><br>a. 这种方法用得最多，而交叉验证评估结果的稳定性很大程度取决于S.<br>b. 与留出法类似，划分为S折存在多种方式，所以为了减小样本划分不同而引入的误差，通常随机使用不同的划分重复P次，即为<strong>P次S折交叉验证</strong><br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def S_cross(item, S):</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    将样本数据分为K-折,再将训练集和测试集组合成<span class="keyword">k</span>组返回.train_test为<span class="keyword">list</span>类型以元组形式表示</span><br><span class="line">    例如[(test1,train1),(test2,train2),(test3,train),(test4.train4)],注意获取S组训练集和测试集</span><br><span class="line">    :param item: sample data</span><br><span class="line">    :<span class="keyword">return</span>: S-折 train <span class="built_in">and</span> test data</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    item = shuffle(item)</span><br><span class="line">    <span class="keyword">k</span> = math.<span class="built_in">floor</span>(<span class="built_in">len</span>(item) / S)</span><br><span class="line">    train_test = []</span><br><span class="line">    item_indexs = [item.<span class="built_in">index</span>[i:i + <span class="keyword">k</span>, ] <span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(item.<span class="built_in">index</span>)-<span class="keyword">k</span>, <span class="keyword">k</span>)]</span><br><span class="line">    <span class="keyword">for</span> test_index in item_index<span class="variable">s:</span></span><br><span class="line">        train_test.<span class="keyword">append</span>((item.<span class="keyword">loc</span>[test_index], item.<span class="keyword">drop</span>(test_index)))</span><br><span class="line">    <span class="keyword">return</span> train_test</span><br></pre></td></tr></table></figure></p><p><strong>留一法</strong><br>方法：它是S折交叉验证的特殊情况，S=N，其中N为数据容量，划分样本方式唯一。<br>注：<br>a. <strong>数据缺乏时使用</strong>，当数据集很大时，<strong>计算开销很大</strong>。<br>b. 由于测试集只有一个样本，<strong>所以模型因样本规模不同而导致的估计偏差比前两种小</strong>。</p><p><strong>自助法</strong><br>方法：以自助采样为基础，对m个样本数据随机挑选1个，放回后再随机挑选1个，重复m次，这样得到了与样本规模一样大小的训练集，测试集中有一部分样本多次出现，有一部分样本不出现，可以做一个简单估计，样本在m次采样中始终没有被采样的$$\lim\limits_{n \rightarrow +\infty} (1-\frac{1}{m})^m \rightarrow \frac{1}{e} \approx  0.368$$，即大约36.8%的样本未出现在训练集中，未出现在训练集中的样本组合成测试集。</p><p><strong>Note：</strong><br>a. <strong>自助法在数据较小，难以划分测试集和训练集时有用</strong>，从初始数据集中产生多个不同的训练集，这对集成学习等方法有很多好处。<br>b.<strong>缺点：自助法产生的数据集改变了初始数据集的分布，引入估计误差。</strong><br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def bootstraping(item):</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    自助法采样.对数据进行M=<span class="built_in">len</span>(item.<span class="built_in">index</span>)有放回采样.最后得到M个训练集,未被采样到的作为测试集</span><br><span class="line">    :param item:sample data</span><br><span class="line">    :<span class="keyword">return</span>:训练集和测试集</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    train_indexs = np.random.choice(item.<span class="built_in">index</span>, size=<span class="built_in">len</span>(item.<span class="built_in">index</span>), replace=True)</span><br><span class="line">    train_data = item.<span class="keyword">loc</span>[train_indexs, ]</span><br><span class="line">    test_data = item.<span class="keyword">drop</span>(<span class="keyword">list</span>(<span class="keyword">set</span>(item.<span class="built_in">index</span>) - <span class="keyword">set</span>(train_indexs)))</span><br><span class="line">    <span class="keyword">return</span> train_data, test_data</span><br></pre></td></tr></table></figure></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>划分训练集和测试是机器学习的基础,在这些划分方法中最常用的就是S折交叉验证,为了以后使用方便,我附带了实现函数.</p><p>如有疑问,联系<a href="mailto:dengwenjun818@gmail.com" target="_blank" rel="noopener">dengwenjun818@gmail.com</a></p>]]></content>
      
      
      <categories>
          
          <category> learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Kmean聚类分析航空公司客户价值</title>
      <link href="/2019/02/25/%E5%9F%BA%E4%BA%8EKmean%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC/"/>
      <url>/2019/02/25/%E5%9F%BA%E4%BA%8EKmean%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<hr><p>layout: post<br>title: 基于Kmean聚类分析航空公司客户价值<br>categories: Case<br>description: 利用无监督方法–聚类分析客户价值<br>keywords: 聚类，客户价值分析</p><hr><p>利用无监督方法–聚类分析客户价值</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>这次的分析课题是我研究生课程数据分析与决策的小组作业，首先感谢我的小伙伴们。<br>这一次数据分析完全是以研究问题驱动，也是我们几次小组讨论之后的结果。在拿到数据《某航空公司客户数据》时，我们计划以一个完整的数据挖掘流程去挖掘数据中的商业价值。流程如下：</p><hr><h5 id="数据预处理-gt-聚类探索-gt-数据可视化-gt-航空公司客户价值分析"><a href="#数据预处理-gt-聚类探索-gt-数据可视化-gt-航空公司客户价值分析" class="headerlink" title="数据预处理-&gt;聚类探索-&gt;数据可视化-&gt;航空公司客户价值分析"></a>数据预处理-&gt;聚类探索-&gt;数据可视化-&gt;航空公司客户价值分析</h5><hr><p>我们因为我主要负责实现客户聚类以及分析，所有还是打算把它写成一个完整的案例。</p><h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><h4 id="1-基础模型-RFM"><a href="#1-基础模型-RFM" class="headerlink" title="1.基础模型(RFM)"></a>1.基础模型(RFM)</h4><p>基于客户价值分析有一个基础模型，即RFM模型(如下图1-1:RFM模型)， RFM是衡量客户价值和客户创利能力的重要工具和手段。在众多的客户关系管理(CRM)的分析模式中，RFM模型是被广泛使用的。该模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱3项指标来描述该客户的价值状况，从而识别高价值的客户，即：<br>Recency： 最近消费时间间隔<br>Frequency： 消费频率<br>Monetary： 消费金额</p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-1b4941d43a457826.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2018-05-01-1.png"></p><p>结合RFM模型我们选取了原始数据中的三个属性值[平均乘机间隔:AVG_INTERVAL,观测窗口内的飞行次数:FLIGHT_COUNT,总票价:SUM_PRICE]三个指标对用户进行聚类，其中总票价是一个新生成的特征：由两年票价总和而来，最终将客户聚为5类。对于RFM模型的聚类结果的类中心（如表1-1：RFM模型聚类的类中心）如下：<br>表1-1：RFM模型聚类的类中心</p><table><thead><tr><th style="text-align:center">Category</th><th style="text-align:center">AVG_INTERVAL</th><th style="text-align:center">FLIGHT_COUNT</th><th style="text-align:center">SUM_PRICE</th></tr></thead><tbody><tr><td style="text-align:center">C_1</td><td style="text-align:center">71.59704</td><td style="text-align:center">9.454219</td><td style="text-align:center">71189422</td></tr><tr><td style="text-align:center">C_2</td><td style="text-align:center">12.11986</td><td style="text-align:center">94</td><td style="text-align:center">1.29E+11</td></tr><tr><td style="text-align:center">C_3</td><td style="text-align:center">18.15115</td><td style="text-align:center">47.36056</td><td style="text-align:center">2.55E+09</td></tr><tr><td style="text-align:center">C_4</td><td style="text-align:center">7.213259</td><td style="text-align:center">108.8333</td><td style="text-align:center">2.55E+09</td></tr><tr><td style="text-align:center">C_5</td><td style="text-align:center">19.92913</td><td style="text-align:center">113.5</td><td style="text-align:center">2.14E+11</td></tr></tbody></table><p>通过对类中心进行列项归一化（归一化的方法为线性转换：y=（x-MinValue）/(MaxValue-MinValue)），然后基于归一化后的数据画出了雷达图（如图1-2：RFM模型的雷达图）。</p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-44f0958840261762.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2018-05-01-2.png"></p><p>从图中可以看到，每一类人数分别是：C_1：54292人，C_2：6人，C_3：3943人，C_4：32，C_5：2两人，从数据和图中可以看出，单纯依靠RFM三个指标分出的5类用户效果不佳，不仅每一类的人数分布差异很大，且C_2，C_5重叠很大。</p><hr><h4 id="2-拓展模型-1（LRFMC）"><a href="#2-拓展模型-1（LRFMC）" class="headerlink" title="2. 拓展模型-1（LRFMC）"></a>2. 拓展模型-1（LRFMC）</h4><p>虽然RFM模型可以衡量客户价值,但是对于这次的航空数据集，在聚类之后对于刻画不同的用户价值的效果并不理想，所以基于RFM模型可能不合适。由于同样的消费金额的不同旅客对航空公司的价值不同，例如买长航线、低等仓的旅客和买短航线、高等仓的旅客消费金额相同，但是价值却是不同的。显然后者更有价值。所以进一步地，选择客户在一定时间内的飞行里程M和乘坐舱位所对应的折扣系数C。同时，因为航空公司会员的加入时间一定程度上可以影响客户价值，所以我们在航空公司客户价值分析模型中添加客户关系长度 L，作为区分客户价值的另一个指标，所以我们构建出LRFMC 模型。</p><hr><p>L：会员入会时间距观测窗口结束的时间（TIME_INTERVAL）<br>R：客户最近一次乘坐公司分级距观测窗口结束的时间（月数）（LAST_TO_END）<br>F：客户在观测窗口内乘坐公司飞机的次数（FLIGHT_COUNT）<br>M：客户在观测窗口内累计的飞行里程（SEG_KM_SUM）<br>C：客户在观测窗口内乘坐舱位所对应的折扣系数的平均值（avg_discount）</p><hr><p>在LRFMC模型中，我们选择的特征包括了五类，分别是时间间隔:TIME_INTERVAL, 观测窗口内的飞行次数:FLIGHT_COUNT, 观测窗口总飞行公里数SEG_KM_SUM, 平均折扣率:avg_discount, 最后一次乘机时间至观测窗口结果时长(月)LAST_TO_END，其中时间间隔为会员入会时间距观测窗口结束的时间，聚类结果如下（表2-1 LRFMC聚类模型的类中心）.</p><p>表2-1 LRFMC聚类模型的类中心</p><table><thead><tr><th style="text-align:center">Category</th><th style="text-align:center">TIME_INTERVAL</th><th style="text-align:center">FLIGHT_COUNT</th><th style="text-align:center">SEG_KM_SUM</th><th style="text-align:center">avg_discount</th><th style="text-align:center">LAST_TO_END</th></tr></thead><tbody><tr><td style="text-align:center">C_1</td><td style="text-align:center">1941.1565</td><td style="text-align:center">51.8939</td><td style="text-align:center">81022.5</td><td style="text-align:center">0.78842</td><td style="text-align:center">27.4025</td></tr><tr><td style="text-align:center">C_2</td><td style="text-align:center">1537.4712</td><td style="text-align:center">15.1758</td><td style="text-align:center">21362.8</td><td style="text-align:center">0.72559</td><td style="text-align:center">97.5557</td></tr><tr><td style="text-align:center">C_3</td><td style="text-align:center">1730.7709</td><td style="text-align:center">30.1751</td><td style="text-align:center">44550.5</td><td style="text-align:center">0.75299</td><td style="text-align:center">49.7244</td></tr><tr><td style="text-align:center">C_4</td><td style="text-align:center">1352.2401</td><td style="text-align:center">4.97864</td><td style="text-align:center">6373.01</td><td style="text-align:center">0.70832</td><td style="text-align:center">231.587</td></tr><tr><td style="text-align:center">C_5</td><td style="text-align:center">2130.6303</td><td style="text-align:center">76.5931</td><td style="text-align:center">152434.</td><td style="text-align:center">0.84164</td><td style="text-align:center">19.4043</td></tr></tbody></table><p>通过对LRFMC聚类结果的类中心进行归一化，我们得到了如下雷达图（图2-1： LRFMC模型雷达图）：</p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-404e1d4cc7946f3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2018-05-01-3.png"></p><p>通过对上图进行观察和分析，我们知道每一类别人数分别为：C_1:1867,C_2:14887,C_3:5831,C_4:35294,C_5:376, 其中C_5在时间间隔、观测窗口内的飞行次数、观测窗口总飞行公里数、平均折扣率、四个维度都比C_1、C_3、C_2更大，在最后一次乘机时间至观测窗口结果时长(月)这个维度上，C_4&gt;C_2&gt;C_3&gt;C_1&gt;C_5，根据雷达图的性质，面积越大，它的价值越大，所以价值大小排序为：C_5&gt;C_1&gt;C_3&gt;C_2&gt;C_4，依据客户价值排序我们可以将其分为重要保持客户、重要发展客户、重要挽留客户、普通价值客户、低价值客户：<br>表2-2:LRFMC模型的客户价值关系</p><table><thead><tr><th style="text-align:center">Category</th><th style="text-align:center">Value Rating</th><th style="text-align:center">Headcount</th><th style="text-align:center">客户价值</th></tr></thead><tbody><tr><td style="text-align:center">C_1</td><td style="text-align:center">2</td><td style="text-align:center">1867</td><td style="text-align:center">重要发展客户</td></tr><tr><td style="text-align:center">C_2</td><td style="text-align:center">4</td><td style="text-align:center">14887</td><td style="text-align:center">普通价值客户</td></tr><tr><td style="text-align:center">C_3</td><td style="text-align:center">3</td><td style="text-align:center">5831</td><td style="text-align:center">重要挽留客户</td></tr><tr><td style="text-align:center">C_4</td><td style="text-align:center">5</td><td style="text-align:center">35294</td><td style="text-align:center">低价值客户</td></tr><tr><td style="text-align:center">C_5</td><td style="text-align:center">1</td><td style="text-align:center">376</td><td style="text-align:center">重要保持客户</td></tr></tbody></table><hr><h4 id="3-拓展模型-2（LRFMC-2P模型）"><a href="#3-拓展模型-2（LRFMC-2P模型）" class="headerlink" title="3 拓展模型-2（LRFMC+2P模型）"></a>3 拓展模型-2（LRFMC+2P模型）</h4><p>在模型LRFMC模型中我们已经将基于五个指标（LRFMC）将用户划分为五类价值客户分别是重要发展客户、重要保持客户、重要挽留客户、普通价值客户、低价值客户，从雷达图可以比较清晰地观测和分析出客户价值，为了进一步探究是否其他属性会对客户价值聚类有正向作用，我们在LRFMC模型基础上增加了总积分Points_Sum和总票价SUM_PRICE两个属性从而提出了LRFMC+2P模型，以生活检验我们知道，总积分和总票价越高，那么客户在平台的消费力也越大，价值显然更高，因此我们提出的LRFMC+2P模型的参数如下：</p><hr><p>L：会员入会时间距观测窗口结束的时间（TIME_INTERVAL）<br>R：客户最近一次乘坐公司分级距观测窗口结束的时间（月数）（LAST_TO_END）<br>F：客户在观测窗口内乘坐公司飞机的次数（FLIGHT_COUNT）<br>M：客户在观测窗口内累计的飞行里程（SEG_KM_SUM）<br>C：客户在观测窗口内乘坐舱位所对应的折扣系数的平均值（avg_discount）<br>2P: 客户在观测期的总积分和总票价（”Points_Sum”, “SUM_PRICE”）</p><hr><p>基于我们提出的LRFMC+2P模型，结合航空数据的属性，我们对其进行聚类分析，得到了相应的聚类结果，其中每类的类中心（表3-4：LRFMC+2P模型的类中心），同时，对类中心进行归一化处理，然后画出了7个属性的雷达图（如图3-4：LRFMC+2P模型的雷达图）：<br>表3-1：LRFMC+2P模型的类中心</p><table><thead><tr><th style="text-align:center">Category</th><th style="text-align:center">TIME_INTERVAL</th><th style="text-align:center">FLIGHT_COUNT</th><th style="text-align:center">SEG_KM_SUM</th><th style="text-align:center">avg_discount</th><th style="text-align:center">LAST_TO_END</th><th style="text-align:center">Points_Sum</th><th style="text-align:center">SUM_PRICE</th></tr></thead><tbody><tr><td style="text-align:center">C_1</td><td style="text-align:center">1424.</td><td style="text-align:center">9.5</td><td style="text-align:center">13534.7</td><td style="text-align:center">0.71191</td><td style="text-align:center">181.5091</td><td style="text-align:center">9259.56</td><td style="text-align:center">72586528</td></tr><tr><td style="text-align:center">C_2</td><td style="text-align:center">2215.167</td><td style="text-align:center">94</td><td style="text-align:center">257112.</td><td style="text-align:center">1.11308</td><td style="text-align:center">20</td><td style="text-align:center">323501</td><td style="text-align:center">1.29E+1</td></tr><tr><td style="text-align:center">C_3</td><td style="text-align:center">1860.25</td><td style="text-align:center">109</td><td style="text-align:center">196685.</td><td style="text-align:center">1.07389</td><td style="text-align:center">6.583333</td><td style="text-align:center">249070.</td><td style="text-align:center">7.68E+1</td></tr><tr><td style="text-align:center">C_4</td><td style="text-align:center">1971.286</td><td style="text-align:center">47.6</td><td style="text-align:center">69385.5</td><td style="text-align:center">0.83994</td><td style="text-align:center">29.41255</td><td style="text-align:center">59803.9</td><td style="text-align:center">2.57E+1</td></tr><tr><td style="text-align:center">C_5</td><td style="text-align:center">1894</td><td style="text-align:center">114</td><td style="text-align:center">353033</td><td style="text-align:center">1.09407</td><td style="text-align:center">17.5</td><td style="text-align:center">393372</td><td style="text-align:center">2.14E+1</td></tr></tbody></table><p><img src="https://upload-images.jianshu.io/upload_images/5274272-042204d3231b8708.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2018-05-01-4.png"></p><p>通过对雷达图进行观察和分析，以及依据雷达图性质，我们可以得到五类客户价值以及人数分布（表3-5：LRFMC+2P模型的客户价值关系），可以明显看出每类价值分布太不均衡，因而并不适合用户刻画用户价值。<br>表3-2:LRFMC+2P模型的客户价值关系</p><table><thead><tr><th style="text-align:center">Category</th><th style="text-align:center">Value Rating</th><th style="text-align:center">Headcount</th><th style="text-align:center">客户价值</th></tr></thead><tbody><tr><td style="text-align:center">C_1</td><td style="text-align:center">5</td><td style="text-align:center">54326</td><td style="text-align:center">低价值客户</td></tr><tr><td style="text-align:center">C_2</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">重要发展价值</td></tr><tr><td style="text-align:center">C_3</td><td style="text-align:center">3</td><td style="text-align:center">12</td><td style="text-align:center">重要挽留客户</td></tr><tr><td style="text-align:center">C_4</td><td style="text-align:center">4</td><td style="text-align:center">3909</td><td style="text-align:center">普通价值客户</td></tr><tr><td style="text-align:center">C_5</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">重要价值客户</td></tr></tbody></table><p>综合以上三个模型，LRFMC的模型更具有实用性，LRFMC模型选择的三个用户属性不仅可以较好刻画用户价值，且每类用户分布较为合理。</p><h3 id="代码附录"><a href="#代码附录" class="headerlink" title="代码附录"></a>代码附录</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env python</span></span><br><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line">from sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> pandas as pd</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> numpy as np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_data(path):<span class="type"></span></span><br><span class="line"><span class="type">    return pd</span>.read_csv(path, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="title">object</span>):</span></span><br><span class="line"><span class="class">    <span class="title">def</span> __<span class="title">init__</span>(<span class="title">self</span>, <span class="title">k</span>):</span></span><br><span class="line"><span class="class">        <span class="title">self</span>.<span class="title">train_data</span> = <span class="title">None</span></span></span><br><span class="line"><span class="class">        <span class="title">self</span>.<span class="title">k_cluster</span> = <span class="title">k</span></span></span><br><span class="line"><span class="class">        <span class="title">self</span>.<span class="title">feature</span> = </span>&#123;<span class="string">"feature_3"</span>: <span class="type"></span>[<span class="string">"AVG_INTERVAL"</span>, <span class="string">"FLIGHT_COUNT"</span>, <span class="string">"SUM_PRICE"</span>],</span><br><span class="line">                        <span class="string">"feature_5"</span>: <span class="type"></span>[<span class="string">"TIME_INTERVAL"</span>, <span class="string">"FLIGHT_COUNT"</span>, <span class="string">"SEG_KM_SUM"</span>, <span class="string">"avg_discount"</span>, <span class="string">"LAST_TO_END"</span>],</span><br><span class="line">                        <span class="string">"feature_7"</span>: <span class="type"></span>[<span class="string">"TIME_INTERVAL"</span>, <span class="string">"FLIGHT_COUNT"</span>, <span class="string">"SEG_KM_SUM"</span>, <span class="string">"avg_discount"</span>, <span class="string">"LAST_TO_END"</span>,</span><br><span class="line">                                      <span class="string">"Points_Sum"</span>, <span class="string">"SUM_PRICE"</span>]&#125;</span><br><span class="line"></span><br><span class="line">    def <span class="keyword">new</span><span class="type">_train_data</span>(self):<span class="type"></span></span><br><span class="line"><span class="type">        new_train_data </span>= self.train_data.drop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"SUM_PRICE"</span>] = <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"SUM_YR_1"</span>] + <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"SUM_YR_2"</span>]</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"LOAD_TIME"</span>] = <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"LOAD_TIME"</span>].apply(lambda x: <span class="type">x</span>.split(<span class="string">"/"</span>))</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"FFP_DATE"</span>] = <span class="keyword">new</span><span class="type">_train_data</span>[<span class="string">"FFP_DATE"</span>].apply(lambda x: <span class="type">x</span>.split(<span class="string">"/"</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">new</span><span class="type">_train_data</span>.index:<span class="type"></span></span><br><span class="line"><span class="type">            year</span>, month, day = int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"LOAD_TIME"</span>][<span class="number">0</span>]), int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"LOAD_TIME"</span>][<span class="number">1</span>]), \</span><br><span class="line">                               int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"LOAD_TIME"</span>][<span class="number">2</span>])</span><br><span class="line">            year_2, month_2, day_2 = int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"FFP_DATE"</span>][<span class="number">0</span>]), \</span><br><span class="line">                                     int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"FFP_DATE"</span>][<span class="number">1</span>]), int(<span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"FFP_DATE"</span>][<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">new</span><span class="type">_train_data</span>.ix[i, <span class="string">"TIME_INTERVAL"</span>] = (datetime.datetime(year, month, day) -</span><br><span class="line">                                                     datetime.datetime(year_2, month_2, day_2)).days</span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self.feature.values():<span class="type"></span></span><br><span class="line"><span class="type">            for fea in item</span>:</span><br><span class="line">                <span class="keyword">if</span> fea not <span class="keyword">in</span> features:<span class="type"></span></span><br><span class="line"><span class="type">                    features</span>.append(fea)</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span> = <span class="keyword">new</span><span class="type">_train_data</span>.dropna()</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span>.to_csv(<span class="string">"../data/raw/target_data.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span><span class="type">_train_data</span></span><br><span class="line"><span class="type"></span></span><br><span class="line"><span class="type"></span>    def cluster(self, target, label):<span class="type"></span></span><br><span class="line"><span class="type">        kmean_model </span>= KMeans(self.k_cluster)</span><br><span class="line">        kmean_model.fit(target)</span><br><span class="line">        <span class="keyword">return</span> [kmean_model.labels_, kmean_model.cluster_centers_]</span><br><span class="line"></span><br><span class="line">    def graph(self, cluster_result, label, key):<span class="type"></span></span><br><span class="line"><span class="type"></span></span><br><span class="line"><span class="type">        </span># 使用ggplot的绘图风格</span><br><span class="line">        plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">        r1 = pd.Series(cluster_result[<span class="number">0</span>]).value_counts()  <span class="meta"># 统计各个类别的数目</span></span><br><span class="line">        r2 = pd.DataFrame(cluster_result[<span class="number">1</span>])</span><br><span class="line">        r2 = r2.apply(lambda x: <span class="type"></span>(x - np.min(x)) / (np.max(x) - np.min(x)))  <span class="meta"># 找出聚类中心</span></span><br><span class="line">        <span class="meta"># 所有簇中心坐标值中最大值和最小值</span></span><br><span class="line">        max = r2.values.max()</span><br><span class="line">        min = r2.values.min()</span><br><span class="line">        r = pd.concat([r2, r1], axis=<span class="number">1</span>)  <span class="meta"># 横向连接（0是纵向），得到聚类中心对应的类别下的数目</span></span><br><span class="line">        r.columns = label + [u<span class="string">'类别数目'</span>]  <span class="meta"># 重命名表头</span></span><br><span class="line"></span><br><span class="line">        <span class="meta"># 绘图</span></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">        ax = fig.add_subplot(<span class="number">111</span>, polar=True)</span><br><span class="line">        center_num = r.values</span><br><span class="line">        feature = label</span><br><span class="line">        N = len(feature)</span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate(center_num):<span class="type"></span></span><br><span class="line"><span class="type">            </span># 设置雷达图的角度，用于平分切开一个圆面</span><br><span class="line">            angles = np.linspace(<span class="number">0</span>, <span class="number">2</span> * np.pi, N, endpoint=False)</span><br><span class="line">            <span class="meta"># 为了使雷达图一圈封闭起来，需要下面的步骤</span></span><br><span class="line">            center = np.concatenate((v[:<span class="type">-1</span>], [v[<span class="number">0</span>]]))</span><br><span class="line">            angles = np.concatenate((angles, [angles[<span class="number">0</span>]]))</span><br><span class="line">            <span class="meta"># 绘制折线图</span></span><br><span class="line">            ax.plot(angles, center, <span class="string">'o-'</span>, linew<span class="type">idth</span>=<span class="number">2</span>, label=<span class="string">"category_%d : %d"</span> % (i + <span class="number">1</span>, v[<span class="number">-1</span>]))</span><br><span class="line">            <span class="meta"># 填充颜色</span></span><br><span class="line">            ax.fill(angles, center, alpha=<span class="number">0.25</span>)</span><br><span class="line">            <span class="meta"># 添加每个特征的标签</span></span><br><span class="line">            ax.set_thetagrids(angles * <span class="number">180</span> / np.pi, feature, fontsize=<span class="number">15</span>)</span><br><span class="line">            <span class="meta"># 设置雷达图的范围</span></span><br><span class="line">            ax.set_ylim(min - <span class="number">0.1</span>, max + <span class="number">0.1</span>)</span><br><span class="line">            <span class="meta"># 添加标题</span></span><br><span class="line">            plt.title(<span class="string">'The '</span> + key +<span class="string">' of users'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">            <span class="meta"># 添加网格线</span></span><br><span class="line">            ax.grid(True)</span><br><span class="line">            <span class="meta"># 设置图例</span></span><br><span class="line">            plt.legend(loc=<span class="string">'upper right'</span>, bbox_to_anchor=(<span class="number">1.3</span>, <span class="number">1.0</span>), ncol=<span class="number">1</span>, fancybox=True, shadow=True)</span><br><span class="line">            <span class="meta"># 显示图形</span></span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def combine_results(target_data, cluster_result, filename):<span class="type"></span></span><br><span class="line"><span class="type">        path </span>= <span class="string">"../data/results/"</span></span><br><span class="line">        <span class="keyword">new</span><span class="type">_data</span> = pd.DataFrame()</span><br><span class="line">        <span class="keyword">new</span><span class="type">_data</span>[<span class="string">"Cluster_Results"</span>] = pd.Series(cluster_result[<span class="number">0</span>], index=target_data.index)</span><br><span class="line">        <span class="keyword">new</span><span class="type">_data</span> = target_data.join(<span class="keyword">new</span><span class="type">_data</span>)</span><br><span class="line">        <span class="keyword">new</span><span class="type">_data</span>.to_csv(path + filename + <span class="string">"_cluster.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line">        cluster_center = pd.DataFrame(cluster_result[<span class="number">1</span>])</span><br><span class="line">        cluster_center.to_csv(path + filename + <span class="string">"_center.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span><span class="type">_data</span></span><br><span class="line"><span class="type"></span></span><br><span class="line"><span class="type"></span>    def run(self):<span class="type"></span></span><br><span class="line"><span class="type">        self</span>.train_data = read_data(<span class="string">"../data/raw/new_data.csv"</span>)</span><br><span class="line">        <span class="keyword">new</span><span class="type">_train_data</span> = self.<span class="keyword">new</span><span class="type">_train_data</span>()</span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> self.feature.items():<span class="type"></span></span><br><span class="line"><span class="type">            cluster_result </span>= self.cluster(target=<span class="keyword">new</span><span class="type">_train_data</span>[value], label=value)</span><br><span class="line">            self.graph(cluster_result, label=value, key=key)</span><br><span class="line">            self.combine_results(target_data=<span class="keyword">new</span><span class="type">_train_data</span>, cluster_result=cluster_result, filename=key)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:<span class="type"></span></span><br><span class="line"><span class="type">    mo </span>= Model(k=<span class="number">5</span>)</span><br><span class="line">    mo.run()</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本次用户聚类是基于最基础的模型RFM，在RFM模型上进行扩展，选择了第一种扩展模型（LRFMC），实际上，此次建模这是对数据的探索。</p><p>如需获取本次案例的数据或者更加详细的分析报告，请下方关注微信公众号</p><p>回复：某航空公司数据 | 某航空公司报告</p>]]></content>
      
      
      <categories>
          
          <category> 案例分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> K-mean </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git 的基本使用汇总</title>
      <link href="/2019/02/25/git%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/"/>
      <url>/2019/02/25/git%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<hr><p>layout: post<br>title: git 的基本使用汇总<br>categories: Git<br>description: git的使用方法汇总<br>keywords: git、 github平台和coding平台</p><hr><p>参考：<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">廖雪峰git教程</a> </p><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>版本控制这个概念不知道大家有没有听说过，不管你是软件开发还是平时写文章或者毕业设计的论文都要有版本管理这个意识，因为我自己在这个上面吃过不少亏。</p><p>git命令的学习可以使自己能够更好完成自己的工作以及版本的管理，这也是我自己最想总结git这个命令的原因。</p><hr><h4 id="git的简介"><a href="#git的简介" class="headerlink" title="git的简介"></a>git的简介</h4><p>Git是目前世界上最先进的分布式版本控制系统（没有之一），在Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至<strong>GitHub</strong>(国外的平台)，国内也有这样的平台(<strong>Coding</strong>)。<br><br>注：可以了解一下<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374027586935cf69c53637d8458c9aec27dd546a6cd6000" target="_blank" rel="noopener">了解分布式和集中式的区别</a><br></p>那怎么形容<strong>git的好处</strong>呢？比如你在写论文的时候会一直不停的更迭版本，如果手工操作，就会出现下面的情况：<br><br><img src="https://upload-images.jianshu.io/upload_images/5274272-b39f2a952d1c4b19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="figure -2 手动版本管理"><br><br>甚至你必须使用一个文件来记录每个版本的变化，非常的不方便，对于涉及到几个人对一个项目的操作时，合并就更加麻烦，而git能够很好的解决和处理这些问题。<p></p><hr><h4 id="git的安装"><a href="#git的安装" class="headerlink" title="git的安装"></a>git的安装</h4><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dwj@ec:~$ git</span><br><span class="line">The program <span class="string">'git'</span> <span class="keyword">is</span> currently <span class="keyword">not</span> installed. You can install <span class="literal">it</span> <span class="keyword">by</span> typing:</span><br><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure><p>查看是否安装：直接在Linux终端输入git，如果没安装就继续输入：<strong>sudo apt-get install git</strong></p><p>安装完成后，还需要最后一步设置，在命令行输入：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dwj@ec:~$ git<span class="built_in"> config </span>--global user.name <span class="string">"Your Name"</span></span><br><span class="line">dwj@ec:~$ git<span class="built_in"> config </span>--global user.email <span class="string">"email@example.com"</span></span><br></pre></td></tr></table></figure></p><p>注意git config命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址</p><hr><h4 id="git-仓库创建"><a href="#git-仓库创建" class="headerlink" title="git 仓库创建"></a>git 仓库创建</h4><p>仓库又叫版本库(repository)，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，包括：每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”<br>创建过程如下：<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 首先选择一个合适的地方创建一个空目录：</span></span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>mkdir learngit</span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>cd learngit</span><br><span class="line"><span class="comment">## 查看目录路径</span></span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>pwd</span><br><span class="line">/dwj/home/learngit</span><br><span class="line"><span class="comment">##  初始化仓库</span></span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>git init</span><br><span class="line">Initialized empty Git repository <span class="keyword">in</span> /dwj/home/learngit/.git/</span><br></pre></td></tr></table></figure></p><p><strong>注：</strong></p><ol><li>mkdir +仓库名：mkdir可以简记make+dir，pwd是查看当前路径</li><li>建立了一个空仓库后，当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，千万不要手动修改这个目录里面的文件，不然很容易将其破坏；如果该目录是隐藏的，用<strong>ls -ah</strong>命令就可以看见。</li></ol><hr><p><strong> 区别工作区与版本库：</strong></p><p>工作区(working directory):就是你在电脑里能看到的目录<br>版本库（Repository）:工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD.</p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-9ba604fb4c25c481.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="figure -3 : 工作间与版本库的关系"></p><hr><h4 id="基本git命令"><a href="#基本git命令" class="headerlink" title="基本git命令"></a>基本git命令</h4><p>基本操作：<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 将文件放到learngit目录下，添加文件,将文件加入缓存区（如上图）</span></span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>git add &lt;file&gt;</span><br><span class="line"><span class="comment">## 用命令git commit告诉Git，把暂存区的所有内容提交到当前分支，-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，以便能从历史记录里方便地找到改动记录（）</span></span><br><span class="line">dwj<span class="variable">@ec</span><span class="symbol">:</span><span class="variable">$ </span>git commit -m <span class="string">"message"</span></span><br></pre></td></tr></table></figure></p><p>注: 每次修改，如果不add到暂存区，那就不会加入到commit中</p><hr><p><strong>其他命令：</strong></p><p><strong>1. git status</strong> 时刻掌握仓库当前的状态</p><p><strong>2. git diff</strong> 查看difference</p><p><strong>3. git log</strong> 查看历史记录，显示从最近到最远的提交日志，如果嫌输出信息太多，看得眼花缭乱的，可以试试加上–pretty=oneline参数，例如：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> <span class="comment">--pretty=oneline</span></span><br><span class="line"><span class="number">3628164</span>fb26d48395383f8f31179f24e0882e1e0 append GPL</span><br><span class="line">ea34578d5496d7dd233c827ed32a8cd576c5ee85 <span class="built_in">add</span> distributed</span><br><span class="line">cb926e7ea50ad11b8f9e909c05226233bf755030 wrote <span class="keyword">a</span> readme <span class="built_in">file</span></span><br></pre></td></tr></table></figure></p><p>注：类似3628164…882e1e0的是commit id（版本号）</p><p><strong>4.git reset –hard HEAD^</strong> 表示回到上一个版本<br>注：在Git中，用HEAD表示当前版本，用HEAD^表示上一个版本，HEAD^^是上上一个版本，更多版本eg：HEAD~100表示 上100个版本。且可以使用git reset –hard commit id(版本号)到指定版本，且版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位。</p><p><strong>5. git reflog</strong> 查看命令历史，重返未来，以便确定要回到未来的哪个版本</p><p><strong>6.git checkout – file</strong> 想直接丢弃工作区的修改，必须加–，没有–，就变成了“切换到另一个分支”的命令</p><p><strong>7.git reset HEAD file</strong> 改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令<strong>git reset HEAD file</strong>，就回到工作区，用6的方法修改。</p><p><strong>8.git rm <file></file></strong> 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit：另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本<strong>git checkout –</strong>,其实是用版本库里的版本替换工作区的版本：</p><h4 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h4><p>省略注册【github】以及github的设置等等。</p><ol><li>增加远程仓库：</li></ol><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git remote <span class="keyword">add</span><span class="bash"> origin 远程仓库名 </span></span><br><span class="line"><span class="comment">## eg 远程仓库名：git@github.com:DWJWendy/Weibo_Spider.git</span></span><br></pre></td></tr></table></figure><ol start="2"><li>推送到远程仓库：</li></ol><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">push</span> -u <span class="built_in">origin</span> master</span><br></pre></td></tr></table></figure><p>注：用git push命令，实际上是把当前分支master推送到远程。由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git之后会把本地的master分支和远程的master分支关联起来，所以之后可以不用-u这个参数，直接 <strong>git push origin master</strong>。</p><ol start="3"><li>克隆远程仓库的内容:<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="keyword">clone</span> <span class="title">远程仓库名</span></span><br></pre></td></tr></table></figure></li></ol><hr><h4 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h4><p>分支管理是我最想整理的一部分，因为现在需要用上，但是自己还不熟/(ㄒoㄒ)/~~</p><table><thead><tr><th>操作</th><th style="text-align:left">代码</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td>查看分支</td><td style="text-align:left">git branch</td><td style="text-align:center">当前分支会在前标*</td></tr><tr><td>创建分支</td><td style="text-align:left">git branch <name></name></td><td style="text-align:center"></td></tr><tr><td>切换分支</td><td style="text-align:left">git checkout <name></name></td><td style="text-align:center"></td></tr><tr><td>创建+切换分支</td><td style="text-align:left">git checkout -b <name></name></td><td style="text-align:center"></td></tr><tr><td>合并某分支到当前分支</td><td style="text-align:left">git merge <name></name></td><td style="text-align:center"></td></tr><tr><td>删除分支</td><td style="text-align:left">git branch -d <name></name></td><td style="text-align:center">合并后删除</td></tr><tr><td>如果要丢弃一个没有被合并过的分支</td><td style="text-align:left">git branch -D <name></name></td><td style="text-align:center">强行删除</td></tr></tbody></table><hr><p>下图是一个分支关系图，每次提交，Git都把它们串成一条时间线，在Git里，master是主分支，在下图中，当我们<strong>创建了一个新的分支dev</strong>时，Git就新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上，我们在dev上的工作完成了，就可以把<strong>dev合并到master上</strong>。Git怎么合并呢？就是直接把master指向dev的当前提交，就完成了合并。合并完分支后，甚至<strong>可以删除dev分支</strong>。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支：</p><h2 id><a href="#" class="headerlink" title></a><img src="https://upload-images.jianshu.io/upload_images/5274272-2bda6d0dc4c9d7b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="figure-4: 分支关系"></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 创建分支并切换到分支</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b dev</span></span><br><span class="line">Switched to a new branch 'dev'</span><br><span class="line">或者</span><br><span class="line"><span class="meta">$</span><span class="bash"> git branch dev</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout dev</span></span><br><span class="line">Switched to branch 'dev'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.查看分支</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git branch</span></span><br><span class="line">* dev</span><br><span class="line">  master</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 3.提交</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add readme.txt </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">"branch test"</span></span></span><br><span class="line">[dev fec145a] branch test</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 4.切换会master</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout master</span></span><br><span class="line">Switched to branch 'master‘</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 5.合并</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git merge dev</span></span><br><span class="line">Updating d17efd8..fec145a</span><br><span class="line">Fast-forward</span><br><span class="line"> readme.txt |    1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 6. 删除分支</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git branch -d dev</span></span><br><span class="line">Deleted branch dev (was fec145a)</span><br></pre></td></tr></table></figure><p>Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。强制禁用Fast forward模式，Git需在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。<br>下面就是–no-ff方式的git merge：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git</span> merge --<span class="literal">no</span>-ff -m <span class="string">"merge with no-ff"</span> dev</span><br></pre></td></tr></table></figure><p><strong>冲突解决</strong>：git告诉我们，文件存在冲突，<strong>必须手动解决冲突后再提交</strong>。<strong>git status</strong>也可以告诉我们冲突的文件。Git会用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改后保存，再提交，用带参数的<strong>git log</strong>也可以看到分支的合并情况</p><hr><p><strong>bug分支</strong>：每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。Git还提供了一个stash功能，可以把当前工作现场“储藏”起来 <strong>git stash</strong>，此时用git status查看工作区，就是干净的，因此可以放心地创建分支来修复bug。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##1. 确定要在哪个分支上修复bug，就在其分支上修复，eg 修复master上的bug，就从master创建临时分支</span></span><br><span class="line">$ git checkout master</span><br><span class="line">Switched <span class="built_in">to</span> branch <span class="string">'master'</span></span><br><span class="line">Your branch is ahead <span class="keyword">of</span> <span class="string">'origin/master'</span> <span class="keyword">by</span> <span class="number">6</span> commits.</span><br><span class="line">$ git checkout -b issue<span class="number">-101</span></span><br><span class="line">Switched <span class="built_in">to</span> <span class="keyword">a</span> <span class="built_in">new</span> branch <span class="string">'issue-101'</span></span><br><span class="line"><span class="comment">## 2. 修复bug后，提交</span></span><br><span class="line"><span class="comment">## 3. 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支</span></span><br></pre></td></tr></table></figure><p>注:<strong>git stash list</strong>可以查看刚才的修复，Git把stash内容存在某个地方了，如需恢复有两个办法：一是用<strong>git stash apply</strong>恢复，但是恢复后，stash内容并不删除，你需要用<strong>git stash drop</strong>来删除；另一种方式是用<strong>git stash pop</strong>，恢复的同时把stash内容也删了：再用git stash list查看，就看不到任何stash内容了，另外你可以多次先用git stash list查看，然后恢复指定的stash，用命令：<strong>git stash apply stash@{0}</strong></p><hr><p><strong>分支策略</strong><br>按照几个基本原则进行分支管理：</p><ol><li><p>master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p></li><li><p>干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本；</p></li><li><p>团队每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了</p></li></ol><hr><p><strong>多人协作</strong></p><table><thead><tr><th>操作</th><th style="text-align:left">代码</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td>查看远程库</td><td style="text-align:left">git remote</td><td style="text-align:center">git remote -v显示详细信息</td></tr><tr><td>推送主分支</td><td style="text-align:left">git push origin master</td><td style="text-align:center">主分支要时刻与远程同步</td></tr><tr><td>推送分支</td><td style="text-align:left">git push origin <dev></dev></td><td style="text-align:center">成员都在dev上工作，因此要与远程同步</td></tr><tr><td>推送Future分支</td><td style="text-align:left">类似</td><td style="text-align:center">每添加一个新功能，最好新建一个feature分支，开发-&gt;合并-&gt;删除</td></tr><tr><td>bug分支</td><td style="text-align:left">类似</td><td style="text-align:center">用于在本地修复bug，就没必要推到远程</td></tr><tr><td>创建远程分支到本地</td><td style="text-align:left">git checkout -b dev origin/dev</td><td style="text-align:center">要在dev分支上开发，就必须创建远程origin的dev分支到本地，用这个命令创建本地dev分支</td></tr></tbody></table><hr><p>我之前在推送代码的时候经常会遇到这样的问题，如下图</p><p><img src="https://upload-images.jianshu.io/upload_images/5274272-4cc51f942b42a7b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="figure -5 : 推送冲突"></p><p>解决策略:解决办法也很简单，Git已经提示我们，先用git pull把最新的提交从origin/dev抓下来，然后，在本地合并，解决冲突，再推送</p><hr><p><strong>小结：</strong></p><p>1、查看远程库信息，使用git remote -v；</p><p>2、本地新建的分支如果不推送到远程，对其他人就是不可见的；</p><p>3、从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交；</p><p>4、在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致；</p><p>5、建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name；</p><p>6、从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。</p><h4 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h4><p>发布一个版本时，通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。</p><table><thead><tr><th>操作</th><th style="text-align:left">代码</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td>打一个新标签</td><td style="text-align:left">git tag <name></name></td><td style="text-align:center">默认为HEAD</td></tr><tr><td>查看标签</td><td style="text-align:left">git tag</td></tr><tr><td>对应的commit id打标签</td><td style="text-align:left">git tag v0.9 6224937</td><td style="text-align:center">数字是commit id</td></tr><tr><td>查看标签信息</td><td style="text-align:left">git show <tagname></tagname></td></tr><tr><td>用PGP签名标签</td><td style="text-align:left">git tag -s <tagname> -m “blablabla…”</tagname></td></tr><tr><td>指定标签信息</td><td style="text-align:left">git tag -a <tagname> -m “blablabla…”</tagname></td></tr><tr><td>删除标签</td><td style="text-align:left">git tag -d v0.1</td></tr><tr><td>推送本地标签</td><td style="text-align:left">git push origin <tagname></tagname></td></tr><tr><td>推送全部本地标签</td><td style="text-align:left">git push origin –tag</td><td style="text-align:center">可以推送全部未推送过的本地标签</td></tr><tr><td>删除远程标签</td><td style="text-align:left">git push origin :refs/tags/<tagname></tagname></td></tr></tbody></table><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>git使用是必须必须会的，每次遇到就是临时的找命令或者只记住了基础的命令，所以想把它总结下来，之后查的时候也方便，最后给需要的你，O(∩_∩)O哈哈~</p>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
